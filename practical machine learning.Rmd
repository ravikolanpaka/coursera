---
title: "Practical Machine Learning"
output: html_document
author: "Ravikiran Kolanpaka"
date: '`r format(Sys.Date(), "%Y-%m-%d")`'
---

## DATA
Download the train and test data from the mentioned sites. 
Upload the files to R. 
Read the data files by interpretting the empty fields and other fields as NA. 
```{r}
setwd("/home/ravikiran/coursera")
pmltraining <- read.csv("pml-training.csv",na.strings = c("NA", "#DIV/0!", ""))
pmltesting <- read.csv("pml-testing.csv",na.strings = c("NA", "#DIV/0!", ""))

```

```{r}
table(pmltraining$classe)
```

If you look at the data the first 6 variables just provide inferences about the data. Hence, They can be removed.

```{r}
pmltraining <- pmltraining[,7:160]
pmltesting <- pmltesting[,7:160]
```

After inputting the data, the variable gyros_forearm_x has been converted to factor variable, when infact it is
numeric. Hence, The variable is converted into numeric variable. 

```{r}
pmltraining$gyros_forearm_x <- as.numeric(pmltraining$gyros_forearm_x)
pmltesting$gyros_forearm_x <- as.numeric(pmltesting$gyros_forearm_x)
```

After looking at the data, Most of the columns have NAs. Hence, Removing them using

```{r}
pmltraining <- pmltraining[ lapply( pmltraining, function(x) sum(is.na(x)) / length(x) ) < 0.4 ]
pmltesting <- pmltesting[ lapply( pmltesting, function(x) sum(is.na(x)) / length(x) ) < 0.4 ]
```

Splitting the training data internally by using the classe column into train and test data by using caret package. Subsample 60% of the 
set into training purposes. Rest 40% of the data can be used as test data. 

```{r}
library(caret)
```

```{r}
set.seed(1234)
inTrain <- createDataPartition(y = pmltraining$classe, p = 0.60,list = FALSE)
train_data <- pmltraining[inTrain,]
test_data <- pmltraining[-inTrain,]
```



# Feature Selection

As the number of Features are 53 , They can be reduced by checking the relative importance of the variables 
using Random Forest varImp method. 

```{r}
library(randomForest)
```

```{r}
set.seed(1234)
rf_model <- randomForest(classe~., data=train_data, importance=TRUE, ntree=100)
varImpPlot(rf_model)
```

By checking the above Accuracy and Gini index graphs. We select the top 10 features required to build the model. 

The 10 Features are:  yaw_belt, roll_belt, num_window, pitch_belt, magnet_dumbbell_y, magnet_dumbbell_z, pitch_forearm, accel_dumbbell_y, roll_arm, and roll_forearm.

These Features will be used to build the model. 

```{r}

set.seed(3141592)
fitModel <- train(classe~roll_belt+yaw_belt+num_window+pitch_belt+magnet_dumbbell_y+magnet_dumbbell_z+pitch_forearm+accel_dumbbell_y+roll_arm+roll_forearm,
                  data=train_data,
                  method="rf",
                  trControl=trainControl(method="cv",number=2),
                  prox=TRUE,
                  verbose=TRUE,
                  allowParallel=TRUE)
```

Prediction on the test data.


```{r}

predictions <- predict(fitModel, newdata=test_data)
confusionMat <- confusionMatrix(predictions, test_data$classe)
confusionMat
```

Prediction on the 20 test cases given by coursera.

```{r}

predictions <- predict(fitModel, newdata=pmltesting)
pmltesting$classe <- predictions
```

```{r}
submit <- data.frame(problem_id = pmltesting$problem_id, classe = predictions)
write.csv(submit, file = "coursera-submission.csv", row.names = FALSE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


